# -*- coding: utf-8 -*-
"""NN(XOR)_sin_librerías.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r7zzPyfOKOlwHJp7d3Th8VdGJn2Ufa3M
"""

import numpy as np
import matplotlib.pyplot as plt

"""XOR """

# A continuación se mostrará un código con el que se entrenaron redes neuronales de 3 capas ocultas con 3 entradas.
inputs = np.array([[0,0,1], [0,1,1], [1,0,1], [1,1,1]]) 
y = np.array([[0], [1], [1] ,[0]])

class NeuralNet(): 
  def __init__ (self, input_size, oculta_size, output_size): 
    self.input_size = input_size 
    self.oculta_size= oculta_size
    self.output_size = output_size 

    self.W1 = np.random.randn(self.input_size, self.oculta_size) 
    self.W2 = np.random.randn(self.oculta_size, self.output_size) 
  def forward(self, X): 
    self.z1 = np.dot(X, self.W1) 
    self.a1 = self.sigmoid(self.z1) # función de activación 
    self.z2 = np.dot(self.a1, self.W2) # De la capa oculta a la salida 
    self.output = self.sigmoid (self.z2) 
    return self.output 

  def backprop(self, X, y, lr): # Se implementa el back propagation y se hace uso de las derivadas parciales
    output = self.forward(X) 
    error_out = output - y 
    delta_out = error_out * self.sigmoid_der(output) 
    derivative_W2 = np.dot(self.a1.T, delta_out) 
    error_oculta_size = np.dot(delta_out, self.W2.T) 
    delta_oculta_size = error_oculta_size * self.sigmoid_der(self.a1) 
    derivative_W1 = np.dot(inputs.T, delta_oculta_size) 

    # Gradiente descendiente 
    self.W2 -= derivative_W2 * lr 
    self.W1 -= derivative_W1 * lr  
    return self.mse(output,y)


  def sigmoid(self, x): 
    return 1/(1 + np.exp(-x)) 
  def sigmoid_der(self, x): 
    return x * (1-x) 
  def mse(self, output, target): 
    return np.mean((output - target) ** 2) / 2

nn = NeuralNet(3,3,1) 
print(nn.forward(inputs)) # Nos hace falta implementar el back propagation

nn = NeuralNet(3,3,1) 
print("Antes:") 
print(nn.forward(inputs)) 
errores = [] 
error = 1 
while error > 0.001: 
  error = nn.backprop(inputs, y, 0.9) 
  errores.append(error) 
print("Después:") 
print(nn.forward(inputs))